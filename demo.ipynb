{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoConfig, AutoModel\n",
    "from hfmodel import MLPConfig, MLPHFModel, WRNConfig, WRNHFModel\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=FutureWarning,\n",
    "    message=\"`resume_download` is deprecated\"\n",
    ")\n",
    "\n",
    "AutoConfig.register(\"mlp\", MLPConfig)\n",
    "AutoModel.register(MLPConfig, MLPHFModel)\n",
    "\n",
    "AutoConfig.register(\"WideResNet\", WRNConfig)\n",
    "AutoModel.register(WRNConfig, WRNHFModel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _show_loss(dataset, model, device=\"cuda:0\"):\n",
    "    loss_fn=torch.nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dataset.set_format(\"torch\")\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    for batch in data_loader:\n",
    "        pass\n",
    "    all_logits = model(batch[\"feature\"].to(device)).detach()\n",
    "    loss = loss_fn(torch.Tensor(all_logits), batch[\"label\"].to(device)).detach().cpu().numpy()\n",
    "    return loss\n",
    "\n",
    "def show_loss(dataset_name, model_name, model_idx=0):\n",
    "    # Load dataset\n",
    "    dataset_path = f\"TDDBench/{dataset_name}\"\n",
    "    dataset = load_dataset(dataset_path)[\"train\"]\n",
    "\n",
    "    # Load target model\n",
    "    model_path = f\"TDDBench/{model_name}-{dataset_name}-{model_idx}\"\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "    # Load training data detection label, 1 means model's training data while 0 means model's non-training data\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    tdd_label = np.array(config.tdd_label)\n",
    "\n",
    "    # Calculate mean training loss and mean non-training loss\n",
    "    train_idxes, nontrain_idxes = np.arange(len(tdd_label))[tdd_label==1], np.arange(len(tdd_label))[tdd_label==0]\n",
    "    train_dataset = dataset.select(indices=train_idxes)\n",
    "    nontrain_dataset = dataset.select(indices=train_idxes)\n",
    "\n",
    "    print(f\"Dataset {dataset_name}, Target model {model_idx}, Model index {model_idx}\")\n",
    "    mean_train_loss = _show_loss(train_dataset, model)\n",
    "    print(f\"Mean cross entropy loss of training samples: {mean_train_loss}\")\n",
    "\n",
    "    mean_nontrain_loss = _show_loss(nontrain_dataset, model)\n",
    "    print(f\"Mean cross entropy loss of non-training samples: {mean_nontrain_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since TDDBench/student couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /data1/home/zhihao/.cache/huggingface/datasets/TDDBench___student/default/0.0.0/c14b433b016b8faa38a6976399b0bc3614afa131 (last modified on Wed Jul 23 14:41:57 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset student, Target model 0, Model index 0\n",
      "Mean cross entropy loss of training samples: 0.40509146451950073\n",
      "Mean cross entropy loss of non-training samples: 0.40509146451950073\n"
     ]
    }
   ],
   "source": [
    "!export HF_ENDPOINT=https://hf-mirror.com\n",
    "show_loss(dataset_name=\"student\", model_name=\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since TDDBench/purchase100 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /data1/home/zhihao/.cache/huggingface/datasets/TDDBench___purchase100/default/0.0.0/d7eaad068444a9b1d3366c698a2bddfd59dec4d6 (last modified on Wed Jul 23 15:09:19 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset purchase100, Target model 1, Model index 1\n",
      "Mean cross entropy loss of training samples: 0.024750620126724243\n",
      "Mean cross entropy loss of non-training samples: 0.024750620126724243\n"
     ]
    }
   ],
   "source": [
    "show_loss(dataset_name=\"purchase100\", model_name=\"mlp\", model_idx=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
